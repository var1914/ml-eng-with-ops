import mlflow
import mlflow.sklearn
from prometheus_client import CollectorRegistry, Gauge, Counter, Histogram, push_to_gateway
import pandas as pd
import numpy as np
import json
import time
from datetime import datetime
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64

from data_quality import DataQualityAssessment
from feature_eng import FeatureEngineeringPipeline

DB_CONFIG = {
    "dbname": "postgres",
    "user": "varunrajput", 
    "password": "yourpassword",
    "host": "host.docker.internal",
    "port": "5432"
}

# Keep your existing imports and constants
SYMBOLS = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT', 'XRPUSDT', 'DOTUSDT', 'AVAXUSDT', 'MATICUSDT', 'LINKUSDT']
BASE_URL = "https://api.binance.com/api/v3/klines"

# MinIO Configuration
MINIO_CONFIG = {
    'endpoint': 'minio:9000',  # Adjust based on your Helm setup
    'access_key': 'admin',  # Change these in production!
    'secret_key': 'admin123',
    'secure': False  # Set to True if using HTTPS
}

class MLMonitoringIntegration:
    def __init__(self, mlflow_tracking_uri="http://mlflow.default:5000", 
                 prometheus_gateway="http://pushgateway-prometheus-pushgateway.ml-monitoring:9091"):
        self.mlflow_tracking_uri = mlflow_tracking_uri
        self.prometheus_gateway = prometheus_gateway
        self.logger = logging.getLogger("ml_monitoring")
        
        # Setup MLflow
        mlflow.set_tracking_uri(mlflow_tracking_uri)
        
        # Setup Prometheus metrics
        self.registry = CollectorRegistry()
        self._setup_prometheus_metrics()
        
    def _setup_prometheus_metrics(self):
        """Initialize Prometheus metrics"""
        # Data Quality Metrics
        self.dq_completeness_ratio = Gauge(
            'data_quality_completeness_ratio', 
            'Data completeness ratio by symbol',
            ['symbol'], registry=self.registry
        )
        
        self.dq_gaps_count = Gauge(
            'data_quality_timestamp_gaps',
            'Number of timestamp gaps by symbol', 
            ['symbol'], registry=self.registry
        )
        
        self.dq_outliers_count = Gauge(
            'data_quality_outliers_total',
            'Total outliers detected by symbol',
            ['symbol'], registry=self.registry
        )
        
        # Feature Engineering Metrics
        self.fe_features_count = Gauge(
            'feature_engineering_features_total',
            'Total features generated by symbol',
            ['symbol'], registry=self.registry
        )
        
        self.fe_processing_time = Histogram(
            'feature_engineering_processing_seconds',
            'Feature engineering processing time',
            ['symbol'], registry=self.registry
        )
        
        self.fe_null_percentage = Gauge(
            'feature_engineering_null_percentage',
            'Percentage of null values in features',
            ['symbol', 'feature_type'], registry=self.registry
        )
        
        # Pipeline Metrics
        self.pipeline_runs_total = Counter(
            'ml_pipeline_runs_total',
            'Total ML pipeline runs',
            ['pipeline_type', 'status'], registry=self.registry
        )

class DataQualityMonitoring(MLMonitoringIntegration):
    def __init__(self, db_config, **kwargs):
        super().__init__(**kwargs)
        self.db_config = db_config
        
    def log_data_quality_metrics(self, quality_results, symbols):
        """Log data quality results to MLflow and Prometheus"""
        
        # Start MLflow experiment
        experiment_name = "data_quality_assessment"
        mlflow.set_experiment(experiment_name)
        
        with mlflow.start_run(run_name=f"dq_assessment_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
            
            # Log overall completeness
            overall_completeness = quality_results['completeness']['avg_completeness']
            mlflow.log_metric("overall_completeness_ratio", overall_completeness)
            
            # Log per-symbol metrics
            for symbol in symbols:
                # Completeness
                symbol_completeness = next(
                    (s['completeness_ratio'] for s in quality_results['completeness']['completeness_summary'] 
                     if s['symbol'] == symbol), 0
                )
                mlflow.log_metric(f"completeness_{symbol}", symbol_completeness)
                self.dq_completeness_ratio.labels(symbol=symbol).set(symbol_completeness)
                
                # Gaps
                gaps_count = quality_results['timestamp_continuity'][symbol]['gaps_found']
                mlflow.log_metric(f"gaps_{symbol}", gaps_count)
                self.dq_gaps_count.labels(symbol=symbol).set(gaps_count)
                
                # Outliers
                outliers_count = quality_results['outlier_detection'][symbol]['total_outliers']
                mlflow.log_metric(f"outliers_{symbol}", outliers_count)
                self.dq_outliers_count.labels(symbol=symbol).set(outliers_count)
            
            # Create and log visualizations
            self._create_dq_visualizations(quality_results, symbols)
            
            # Log raw results as artifact
            mlflow.log_dict(quality_results, "data_quality_results.json")
            
            # Update pipeline counter
            self.pipeline_runs_total.labels(pipeline_type="data_quality", status="success").inc()
            
        # Push metrics to Prometheus
        try:
            push_to_gateway(self.prometheus_gateway, job='ml_data_quality', 
                          registry=self.registry)
            self.logger.info("Pushed data quality metrics to Prometheus")
        except Exception as e:
            self.logger.error(f"Failed to push to Prometheus: {e}")
    
    def _create_dq_visualizations(self, quality_results, symbols):
        """Create data quality visualizations"""
        
        # 1. Completeness Overview
        completeness_data = []
        for summary in quality_results['completeness']['completeness_summary']:
            completeness_data.append({
                'symbol': summary['symbol'],
                'completeness_ratio': summary['completeness_ratio'],
                'record_count': summary['record_count']
            })
        
        df_completeness = pd.DataFrame(completeness_data)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Completeness bar chart
        axes[0,0].bar(df_completeness['symbol'], df_completeness['completeness_ratio'])
        axes[0,0].set_title('Data Completeness by Symbol')
        axes[0,0].set_ylabel('Completeness Ratio')
        axes[0,0].tick_params(axis='x', rotation=45)
        
        # Record count
        axes[0,1].bar(df_completeness['symbol'], df_completeness['record_count'])
        axes[0,1].set_title('Record Count by Symbol') 
        axes[0,1].set_ylabel('Record Count')
        axes[0,1].tick_params(axis='x', rotation=45)
        
        # Gaps heatmap
        gaps_data = []
        outliers_data = []
        for symbol in symbols:
            gaps_data.append(quality_results['timestamp_continuity'][symbol]['gaps_found'])
            outliers_data.append(quality_results['outlier_detection'][symbol]['total_outliers'])
        
        issues_df = pd.DataFrame({
            'Symbol': symbols,
            'Gaps': gaps_data,
            'Outliers': outliers_data
        }).set_index('Symbol')
        
        sns.heatmap(issues_df.T, annot=True, fmt='d', ax=axes[1,0], cmap='Reds')
        axes[1,0].set_title('Data Quality Issues Heatmap')
        
        # Quality score (combined metric)
        quality_scores = []
        for symbol in symbols:
            completeness = next(
                (s['completeness_ratio'] for s in quality_results['completeness']['completeness_summary'] 
                 if s['symbol'] == symbol), 0
            )
            gaps = quality_results['timestamp_continuity'][symbol]['gaps_found']
            outliers = quality_results['outlier_detection'][symbol]['total_outliers']
            
            # Simple quality score (0-100)
            quality_score = max(0, completeness * 100 - gaps * 0.5 - outliers * 0.1)
            quality_scores.append(quality_score)
        
        axes[1,1].bar(symbols, quality_scores, color=['green' if s > 80 else 'orange' if s > 60 else 'red' for s in quality_scores])
        axes[1,1].set_title('Overall Quality Score by Symbol')
        axes[1,1].set_ylabel('Quality Score (0-100)')
        axes[1,1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        mlflow.log_figure(fig, "data_quality_overview.png")
        plt.close()

class FeatureEngineeringMonitoring(MLMonitoringIntegration):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
    def log_feature_engineering_metrics(self, feature_results, symbols):
        """Log feature engineering results to MLflow and Prometheus"""
        
        experiment_name = "feature_engineering"
        mlflow.set_experiment(experiment_name)
        
        with mlflow.start_run(run_name=f"fe_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
            
            # Log per-symbol feature metrics
            for symbol in symbols:
                features_df = feature_results['feature_data'][symbol]
                
                # Basic metrics
                total_features = len(features_df.columns)
                total_records = len(features_df)
                
                mlflow.log_metric(f"features_count_{symbol}", total_features)
                mlflow.log_metric(f"records_count_{symbol}", total_records)
                
                # Set Prometheus metrics
                self.fe_features_count.labels(symbol=symbol).set(total_features)
                
                # Feature type analysis
                feature_types = self._categorize_features(features_df.columns)
                for ftype, count in feature_types.items():
                    mlflow.log_metric(f"{ftype}_features_{symbol}", count)
                
                # Null value analysis
                null_percentages = (features_df.isnull().sum() / len(features_df) * 100)
                avg_null_pct = null_percentages.mean()
                
                mlflow.log_metric(f"avg_null_percentage_{symbol}", avg_null_pct)
                
                # Log null percentages by feature type
                for ftype in feature_types.keys():
                    ftype_cols = [col for col in features_df.columns if self._get_feature_type(col) == ftype]
                    if ftype_cols:
                        ftype_null_pct = null_percentages[ftype_cols].mean()
                        self.fe_null_percentage.labels(symbol=symbol, feature_type=ftype).set(ftype_null_pct)
                
                # Feature correlation analysis
                corr_matrix = features_df.select_dtypes(include=[np.number]).corr()
                high_corr_pairs = self._find_high_correlations(corr_matrix)
                mlflow.log_metric(f"high_correlation_pairs_{symbol}", len(high_corr_pairs))
                
            # Create visualizations
            self._create_fe_visualizations(feature_results, symbols)
            
            # Log feature importance (if available)
            self._log_feature_statistics(feature_results, symbols)
            
            # Update pipeline counter
            self.pipeline_runs_total.labels(pipeline_type="feature_engineering", status="success").inc()
        
        # Push to Prometheus
        try:
            push_to_gateway(self.prometheus_gateway, job='ml_feature_engineering', 
                          registry=self.registry)
            self.logger.info("Pushed feature engineering metrics to Prometheus")
        except Exception as e:
            self.logger.error(f"Failed to push to Prometheus: {e}")
    
    def _categorize_features(self, columns):
        """Categorize features by type"""
        categories = {
            'technical': 0,
            'price': 0, 
            'volume': 0,
            'time': 0,
            'cross_symbol': 0,
            'other': 0
        }
        
        for col in columns:
            ftype = self._get_feature_type(col)
            categories[ftype] += 1
            
        return categories
    
    def _get_feature_type(self, column_name):
        """Determine feature type from column name"""
        col = column_name.lower()
        
        if any(x in col for x in ['rsi', 'macd', 'sma', 'ema', 'bb_', 'atr']):
            return 'technical'
        elif any(x in col for x in ['return', 'volatility', 'price', 'high', 'low', 'close', 'open']):
            return 'price'
        elif any(x in col for x in ['volume', 'obv', 'vwap']):
            return 'volume'
        elif any(x in col for x in ['hour', 'day', 'month', 'weekend', 'session']):
            return 'time'
        elif any(x in col for x in ['corr_', 'relative_', 'ratio_to_', 'btc']):
            return 'cross_symbol'
        else:
            return 'other'
    
    def _find_high_correlations(self, corr_matrix, threshold=0.8):
        """Find highly correlated feature pairs"""
        high_corr_pairs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = abs(corr_matrix.iloc[i, j])
                if corr_val > threshold and not pd.isna(corr_val):
                    high_corr_pairs.append((
                        corr_matrix.columns[i], 
                        corr_matrix.columns[j], 
                        corr_val
                    ))
        return high_corr_pairs
    
    def _create_fe_visualizations(self, feature_results, symbols):
        """Create feature engineering visualizations"""
        
        # Feature count comparison
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Feature count by symbol
        feature_counts = []
        for symbol in symbols:
            feature_counts.append(len(feature_results['feature_data'][symbol].columns))
        
        axes[0,0].bar(symbols, feature_counts)
        axes[0,0].set_title('Feature Count by Symbol')
        axes[0,0].set_ylabel('Number of Features')
        
        # 2. Feature type distribution (using first symbol as example)
        first_symbol = symbols[0]
        feature_types = self._categorize_features(feature_results['feature_data'][first_symbol].columns)
        
        axes[0,1].pie(feature_types.values(), labels=feature_types.keys(), autopct='%1.1f%%')
        axes[0,1].set_title(f'Feature Type Distribution ({first_symbol})')
        
        # 3. Null value heatmap
        null_data = []
        for symbol in symbols:
            df = feature_results['feature_data'][symbol]
            null_pct = (df.isnull().sum() / len(df) * 100).mean()
            null_data.append(null_pct)
        
        axes[1,0].bar(symbols, null_data, color=['red' if x > 10 else 'orange' if x > 5 else 'green' for x in null_data])
        axes[1,0].set_title('Average Null Percentage by Symbol')
        axes[1,0].set_ylabel('Null Percentage (%)')
        
        # 4. Feature correlation summary
        high_corr_counts = []
        for symbol in symbols:
            df = feature_results['feature_data'][symbol]
            corr_matrix = df.select_dtypes(include=[np.number]).corr()
            high_corr_pairs = self._find_high_correlations(corr_matrix)
            high_corr_counts.append(len(high_corr_pairs))
        
        axes[1,1].bar(symbols, high_corr_counts)
        axes[1,1].set_title('High Correlation Feature Pairs')
        axes[1,1].set_ylabel('Number of High Corr Pairs (>0.8)')
        
        plt.tight_layout()
        mlflow.log_figure(fig, "feature_engineering_overview.png")
        plt.close()
        
        # Create detailed correlation heatmap for first symbol
        self._create_correlation_heatmap(feature_results['feature_data'][symbols[0]], symbols[0])
    
    def _create_correlation_heatmap(self, df, symbol):
        """Create correlation heatmap for features"""
        # Select subset of features for readability
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        sample_cols = numeric_cols[:20] if len(numeric_cols) > 20 else numeric_cols
        
        plt.figure(figsize=(12, 10))
        corr_matrix = df[sample_cols].corr()
        
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', 
                   cmap='coolwarm', center=0, square=True)
        plt.title(f'Feature Correlation Matrix - {symbol} (Sample)')
        plt.tight_layout()
        
        mlflow.log_figure(plt.gcf(), f"correlation_matrix_{symbol}.png")
        plt.close()
    
    def _log_feature_statistics(self, feature_results, symbols):
        """Log detailed feature statistics"""
        for symbol in symbols:
            df = feature_results['feature_data'][symbol]
            
            # Basic statistics for numeric features
            numeric_df = df.select_dtypes(include=[np.number])
            stats = numeric_df.describe()
            
            # Log as artifact
            stats_dict = stats.to_dict()
            mlflow.log_dict(stats_dict, f"feature_statistics_{symbol}.json")
            
            # Log key statistics as metrics
            mlflow.log_metric(f"feature_mean_std_{symbol}", stats.loc['std'].mean())
            mlflow.log_metric(f"feature_mean_skew_{symbol}", numeric_df.skew().mean())

# Usage example:
# Initialize monitoring
dq_monitor = DataQualityMonitoring(
    db_config=DB_CONFIG,
    mlflow_tracking_uri="http://mlflow.default:5000",
    prometheus_gateway="http://pushgateway-prometheus-pushgateway.ml-monitoring:9091"
)

fe_monitor = FeatureEngineeringMonitoring(
    mlflow_tracking_uri="http://mlflow.default:5000",
    prometheus_gateway="http://pushgateway-prometheus-pushgateway.ml-monitoring:9091"
)

# Run data quality assessment and log results
assessor = DataQualityAssessment(DB_CONFIG)
quality_results = assessor.run_quality_assessment(SYMBOLS)
dq_monitor.log_data_quality_metrics(quality_results, SYMBOLS)

# Run feature engineering and log results  
pipeline = FeatureEngineeringPipeline(DB_CONFIG)
feature_results = pipeline.run_feature_pipeline(SYMBOLS)
fe_monitor.log_feature_engineering_metrics(feature_results, SYMBOLS)