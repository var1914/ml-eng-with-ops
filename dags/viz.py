import mlflow
import mlflow.sklearn
from prometheus_client import CollectorRegistry, Gauge, Counter, Histogram, push_to_gateway
import pandas as pd
import numpy as np
import json
import time
from datetime import datetime
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64

from data_quality import DataQualityAssessment
from feature_eng import FeatureEngineeringPipeline

import psycopg2

from minio import Minio
from minio.error import S3Error

# Keep your existing imports and constants
SYMBOLS = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT', 'XRPUSDT', 'DOTUSDT', 'AVAXUSDT', 'MATICUSDT', 'LINKUSDT']
BASE_URL = "https://api.binance.com/api/v3/klines"
BUCKET_NAME = 'crypto-features'

# MinIO Configuration
MINIO_CONFIG = {
    'endpoint': 'minio:9000',  # Adjust based on your Helm setup
    'access_key': 'admin',  # Change these in production!
    'secret_key': 'admin123',
    'secure': False  # Set to True if using HTTPS
}

class MLMonitoringIntegration:
    def __init__(self, mlflow_tracking_uri="http://mlflow-tracking", 
                 prometheus_gateway="http://pushgateway-prometheus-pushgateway.ml-monitoring:9091"):
        self.mlflow_tracking_uri = mlflow_tracking_uri
        self.artifact_bucket = BUCKET_NAME
        self.prometheus_gateway = prometheus_gateway
        self.logger = logging.getLogger("ml_monitoring")
        
        # Setup MLflow
        mlflow.set_tracking_uri(mlflow_tracking_uri)

        self.minio_client = self._get_minio_client()
        self._ensure_bucket_exists()
        # Setup Prometheus metrics
        self.registry = CollectorRegistry()
        self._setup_prometheus_metrics()
    
    def _get_minio_client(self):
        try:
            client = Minio(
                MINIO_CONFIG['endpoint'],
                access_key=MINIO_CONFIG['access_key'],
                secret_key=MINIO_CONFIG['secret_key'],
                secure=MINIO_CONFIG['secure']
            )
            self.logger.info(f"MinIO Client Initialised")
            return client
        except Exception as e:
            self.logger.error(f"Failed to initialise MinIO Client: {str(e)}")
            raise

    def _ensure_bucket_exists(self):
        """Create bucket if it doesn't exist"""
        try:
            if not self.minio_client.bucket_exists(self.artifact_bucket):
                self.minio_client.make_bucket(self.artifact_bucket)
                self.logger.info(f"Created bucket: {self.artifact_bucket}")
            else:
                self.logger.info(f"Bucket {self.artifact_bucket} already exists")
        except S3Error as e:
            self.logger.error(f"Error with bucket operations: {str(e)}")
            raise
        
    def _setup_prometheus_metrics(self):
        """Initialize Prometheus metrics"""
        # Data Quality Metrics
        self.dq_completeness_ratio = Gauge(
            'data_quality_completeness_ratio', 
            'Data completeness ratio by symbol',
            ['symbol'], registry=self.registry
        )
        
        self.dq_gaps_count = Gauge(
            'data_quality_timestamp_gaps',
            'Number of timestamp gaps by symbol', 
            ['symbol'], registry=self.registry
        )
        
        self.dq_outliers_count = Gauge(
            'data_quality_outliers_total',
            'Total outliers detected by symbol',
            ['symbol'], registry=self.registry
        )
        
        # Feature Engineering Metrics
        self.fe_features_count = Gauge(
            'feature_engineering_features_total',
            'Total features generated by symbol',
            ['symbol'], registry=self.registry
        )
        
        self.fe_processing_time = Histogram(
            'feature_engineering_processing_seconds',
            'Feature engineering processing time',
            ['symbol'], registry=self.registry
        )
        
        self.fe_null_percentage = Gauge(
            'feature_engineering_null_percentage',
            'Percentage of null values in features',
            ['symbol', 'feature_type'], registry=self.registry
        )
        
        # Pipeline Metrics
        self.pipeline_runs_total = Counter(
            'ml_pipeline_runs_total',
            'Total ML pipeline runs',
            ['pipeline_type', 'status'], registry=self.registry
        )

        self.validation_pass_rate = Gauge(
            'data_validation_pass_rate',
            'Data validation pass rate by dataset',
            ['dataset_name', 'symbol'], registry=self.registry
        )
        
        self.validation_critical_failures = Gauge(
            'data_validation_critical_failures',
            'Number of critical validation failures',
            ['dataset_name', 'symbol'], registry=self.registry
        )
        
        self.validation_warnings = Gauge(
            'data_validation_warnings',
            'Number of validation warnings',
            ['dataset_name', 'symbol'], registry=self.registry
        )
        
        self.validation_runs_total = Counter(
            'data_validation_runs_total',
            'Total validation runs',
            ['dataset_name', 'status'], registry=self.registry
        )

        # NEW: Overall pipeline health
        self.pipeline_health_score = Gauge(
            'ml_pipeline_health_score',
            'Overall ML pipeline health score',
            ['symbol'], registry=self.registry
        )
        
        self.data_freshness_hours = Gauge(
            'data_freshness_hours',
            'Hours since latest data point',
            ['symbol'], registry=self.registry
        )

      
class DataQualityMonitoring(MLMonitoringIntegration):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
    def log_data_quality_metrics(self, quality_results, symbols):
        """Log data quality results to MLflow and Prometheus"""

        # Clean up any existing MLflow state
        try:
            mlflow.end_run()
        except:
            pass
        
        # Check if experiment exists, create if not
        experiment_name = "data_quality_assessment"
        try:
            experiment = mlflow.get_experiment_by_name(experiment_name)
            if experiment is None:
                mlflow.create_experiment(experiment_name)
                self.logger.info(f"Created new MLflow experiment: {experiment_name}")
            else:
                self.logger.info(f"Using existing MLflow experiment: {experiment_name}")
        except Exception as e:
            self.logger.warning(f"Error checking MLflow experiment: {e}, creating new one")
            mlflow.create_experiment(experiment_name)

        mlflow.set_experiment(experiment_name)

        # Start fresh run
        run = mlflow.start_run(run_name=f"dq_assessment_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        run_id = run.info.run_id
        
        try:
            self.logger.info(f"Started MLflow run: {run_id}")
            
            # Log overall completeness
            overall_completeness = quality_results['completeness']['avg_completeness']
            mlflow.log_metric("overall_completeness_ratio", overall_completeness)
            self.logger.info("Logged overall completeness")
            
            # Log per-symbol metrics
            for symbol in symbols:
                # Your existing metric logging code...
                symbol_completeness = next(
                    (s['completeness_ratio'] for s in quality_results['completeness']['completeness_summary'] 
                    if s['symbol'] == symbol), 0
                )
                mlflow.log_metric(f"completeness_{symbol}", symbol_completeness)
                self.dq_completeness_ratio.labels(symbol=symbol).set(symbol_completeness)
                
                gaps_count = quality_results['timestamp_continuity'][symbol]['gaps_found']
                mlflow.log_metric(f"gaps_{symbol}", gaps_count)
                self.dq_gaps_count.labels(symbol=symbol).set(gaps_count)
                
                outliers_count = quality_results['outlier_detection'][symbol]['total_outliers']
                mlflow.log_metric(f"outliers_{symbol}", outliers_count)
                self.dq_outliers_count.labels(symbol=symbol).set(outliers_count)
                
                self.logger.info(f"Logged metrics for symbol: {symbol}")
            
            # Create visualizations and save to MinIO directly
            try:
                self.logger.info("Starting visualization creation")
                self._create_dq_visualizations(quality_results, symbols, run_id)
                self.logger.info("Completed visualization creation")
            except Exception as viz_error:
                self.logger.error(f"Visualization creation failed: {viz_error}")
            
            # Save raw results to MinIO directly
            try:
                self._save_results_to_minio(quality_results, run_id)
                self.logger.info("Saved raw results to MinIO")
            except Exception as save_error:
                self.logger.error(f"Failed to save results to MinIO: {save_error}")
            
            # Update pipeline counter
            self.pipeline_runs_total.labels(pipeline_type="data_quality", status="success").inc()
            
        except Exception as mlflow_error:
            self.logger.error(f"MLflow logging failed: {mlflow_error}")
            raise
        finally:
            # Explicitly end the run
            try:
                mlflow.end_run()
                self.logger.info("Ended MLflow run")
            except Exception as e:
                self.logger.warning(f"Error ending MLflow run: {e}")
        
        # Push metrics to Prometheus
        try:
            push_to_gateway(self.prometheus_gateway, job='ml_data_quality', 
                        registry=self.registry)
            self.logger.info("Pushed data quality metrics to Prometheus")
        except Exception as e:
            self.logger.error(f"Failed to push to Prometheus: {e}")

    def _save_results_to_minio(self, quality_results, run_id):
        """Save quality results JSON to MinIO"""
        
        # Convert to JSON bytes
        json_data = json.dumps(quality_results, indent=2, default=str)
        json_buffer = BytesIO(json_data.encode('utf-8'))
        
        # Save to MinIO
        object_name = f"mlflow-runs/{run_id}/artifacts/data_quality_results.json"
        self.minio_client.put_object(
            bucket_name=self.artifact_bucket,
            object_name=object_name,
            data=json_buffer,
            length=len(json_data.encode('utf-8')),
            content_type='application/json'
        )
    
    def _create_dq_visualizations(self, quality_results, symbols, run_id):
        """Create data quality visualizations"""
        
        # 1. Completeness Overview
        completeness_data = []
        for summary in quality_results['completeness']['completeness_summary']:
            completeness_data.append({
                'symbol': summary['symbol'],
                'completeness_ratio': summary['completeness_ratio'],
                'record_count': summary['record_count']
            })
        
        df_completeness = pd.DataFrame(completeness_data)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Completeness bar chart
        axes[0,0].bar(df_completeness['symbol'], df_completeness['completeness_ratio'])
        axes[0,0].set_title('Data Completeness by Symbol')
        axes[0,0].set_ylabel('Completeness Ratio')
        axes[0,0].tick_params(axis='x', rotation=45)
        
        # Record count
        axes[0,1].bar(df_completeness['symbol'], df_completeness['record_count'])
        axes[0,1].set_title('Record Count by Symbol') 
        axes[0,1].set_ylabel('Record Count')
        axes[0,1].tick_params(axis='x', rotation=45)
        
        # Gaps heatmap
        gaps_data = []
        outliers_data = []
        for symbol in symbols:
            gaps_data.append(quality_results['timestamp_continuity'][symbol]['gaps_found'])
            outliers_data.append(quality_results['outlier_detection'][symbol]['total_outliers'])
        
        issues_df = pd.DataFrame({
            'Symbol': symbols,
            'Gaps': gaps_data,
            'Outliers': outliers_data
        }).set_index('Symbol')
        
        sns.heatmap(issues_df.T, annot=True, fmt='d', ax=axes[1,0], cmap='Reds')
        axes[1,0].set_title('Data Quality Issues Heatmap')
        
        # Quality score (combined metric)
        quality_scores = []
        for symbol in symbols:
            completeness = next(
                (s['completeness_ratio'] for s in quality_results['completeness']['completeness_summary'] 
                 if s['symbol'] == symbol), 0
            )
            gaps = quality_results['timestamp_continuity'][symbol]['gaps_found']
            outliers = quality_results['outlier_detection'][symbol]['total_outliers']
            
            # Simple quality score (0-100)
            quality_score = max(0, completeness * 100 - gaps * 0.5 - outliers * 0.1)
            quality_scores.append(quality_score)
        
        axes[1,1].bar(symbols, quality_scores, color=['green' if s > 80 else 'orange' if s > 60 else 'red' for s in quality_scores])
        axes[1,1].set_title('Overall Quality Score by Symbol')
        axes[1,1].set_ylabel('Quality Score (0-100)')
        axes[1,1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        # Save to MinIO instead of MLflow
        img_buffer = BytesIO()
        fig.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')
        img_buffer.seek(0)
        
        object_name = f"mlflow-runs/{run_id}/artifacts/data_quality_overview.png"
        self.minio_client.put_object(
            bucket_name=self.artifact_bucket,
            object_name=object_name,
            data=img_buffer,
            length=len(img_buffer.getvalue()),
            content_type='image/png'
        )
        
        plt.close()
        self.logger.info(f"Saved visualization to MinIO: {object_name}")

class FeatureEngineeringMonitoring(MLMonitoringIntegration):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
    def log_feature_engineering_metrics(self, feature_results, symbols):
        """Log feature engineering results to MLflow and Prometheus"""
        
        # Clean up any existing MLflow state
        try:
            mlflow.end_run()
        except:
            pass
        
        # Check if experiment exists, create if not
        experiment_name = "feature_engineering"
        try:
            experiment = mlflow.get_experiment_by_name(experiment_name)
            if experiment is None:
                mlflow.create_experiment(experiment_name)
                self.logger.info(f"Created new MLflow experiment: {experiment_name}")
            else:
                self.logger.info(f"Using existing MLflow experiment: {experiment_name}")
        except Exception as e:
            self.logger.warning(f"Error checking MLflow experiment: {e}, creating new one")
            mlflow.create_experiment(experiment_name)

        mlflow.set_experiment(experiment_name)
        
        # Start fresh run
        run = mlflow.start_run(run_name=f"fe_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        run_id = run.info.run_id
        
        try:
            self.logger.info(f"Started MLflow run: {run_id}")
            
            # Log per-symbol feature metrics
            for symbol in symbols:
                parquet_file_path = feature_results['storage_paths'][symbol]
                response = self.minio_client.get_object(self.artifact_bucket, parquet_file_path)
                features_df = pd.read_parquet(BytesIO(response.read()))
                response.close()
                response.release_conn()
                # Basic metrics
                total_features = len(features_df.columns)
                total_records = len(features_df)
                
                mlflow.log_metric(f"features_count_{symbol}", total_features)
                mlflow.log_metric(f"records_count_{symbol}", total_records)
                
                # Set Prometheus metrics
                self.fe_features_count.labels(symbol=symbol).set(total_features)
                
                # Feature type analysis
                feature_types = self._categorize_features(features_df.columns)
                for ftype, count in feature_types.items():
                    mlflow.log_metric(f"{ftype}_features_{symbol}", count)
                
                # Null value analysis
                null_percentages = (features_df.isnull().sum() / len(features_df) * 100)
                avg_null_pct = null_percentages.mean()
                
                mlflow.log_metric(f"avg_null_percentage_{symbol}", avg_null_pct)
                
                # Log null percentages by feature type
                for ftype in feature_types.keys():
                    ftype_cols = [col for col in features_df.columns if self._get_feature_type(col) == ftype]
                    if ftype_cols:
                        ftype_null_pct = null_percentages[ftype_cols].mean()
                        self.fe_null_percentage.labels(symbol=symbol, feature_type=ftype).set(ftype_null_pct)
                
                # Feature correlation analysis
                corr_matrix = features_df.select_dtypes(include=[np.number]).corr()
                high_corr_pairs = self._find_high_correlations(corr_matrix)
                mlflow.log_metric(f"high_correlation_pairs_{symbol}", len(high_corr_pairs))
                
            # Create visualizations
            self._create_fe_visualizations(feature_results, symbols, run_id)
            
            # Log feature importance (if available)
            self._log_feature_statistics(feature_results, symbols, run_id)
            
            # Save raw results to MinIO directly
            try:
                self._save_results_to_minio(feature_results, run_id)
                self.logger.info("Saved raw results to MinIO")
            except Exception as save_error:
                self.logger.error(f"Failed to save results to MinIO: {save_error}")
            
            # Update pipeline counter
            self.pipeline_runs_total.labels(pipeline_type="feature_engineering", status="success").inc()

        except Exception as mlflow_error:
            self.logger.error(f"MLflow logging failed: {mlflow_error}")
            raise
        finally:
            # Explicitly end the run
            try:
                mlflow.end_run()
                self.logger.info("Ended MLflow run")
            except Exception as e:
                self.logger.warning(f"Error ending MLflow run: {e}")
        
        # Push to Prometheus
        try:
            push_to_gateway(self.prometheus_gateway, job='ml_feature_engineering', 
                          registry=self.registry)
            self.logger.info("Pushed feature engineering metrics to Prometheus")
        except Exception as e:
            self.logger.error(f"Failed to push to Prometheus: {e}")
    
    def _save_results_to_minio(self, feature_results, run_id):
        """Save quality results JSON to MinIO"""
        
        # Convert to JSON bytes
        json_data = json.dumps(feature_results, indent=2, default=str)
        json_buffer = BytesIO(json_data.encode('utf-8'))
        
        # Save to MinIO
        object_name = f"mlflow-runs/{run_id}/artifacts/feature_engineering_results.json"
        self.minio_client.put_object(
            bucket_name=self.artifact_bucket,
            object_name=object_name,
            data=json_buffer,
            length=len(json_data.encode('utf-8')),
            content_type='application/json'
        )

    def _categorize_features(self, columns):
        """Categorize features by type"""
        categories = {
            'technical': 0,
            'price': 0, 
            'volume': 0,
            'time': 0,
            'cross_symbol': 0,
            'other': 0
        }
        
        for col in columns:
            ftype = self._get_feature_type(col)
            categories[ftype] += 1
            
        return categories
    
    def _get_feature_type(self, column_name):
        """Determine feature type from column name"""
        col = column_name.lower()
        
        if any(x in col for x in ['rsi', 'macd', 'sma', 'ema', 'bb_', 'atr']):
            return 'technical'
        elif any(x in col for x in ['return', 'volatility', 'price', 'high', 'low', 'close', 'open']):
            return 'price'
        elif any(x in col for x in ['volume', 'obv', 'vwap']):
            return 'volume'
        elif any(x in col for x in ['hour', 'day', 'month', 'weekend', 'session']):
            return 'time'
        elif any(x in col for x in ['corr_', 'relative_', 'ratio_to_', 'btc']):
            return 'cross_symbol'
        else:
            return 'other'
    
    def _find_high_correlations(self, corr_matrix, threshold=0.8):
        """Find highly correlated feature pairs"""
        high_corr_pairs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = abs(corr_matrix.iloc[i, j])
                if corr_val > threshold and not pd.isna(corr_val):
                    high_corr_pairs.append((
                        corr_matrix.columns[i], 
                        corr_matrix.columns[j], 
                        corr_val
                    ))
        return high_corr_pairs
    
    def _create_fe_visualizations(self, feature_results, symbols, run_id):
        """Create feature engineering visualizations"""
        
        # Feature count comparison
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Feature count by symbol - FIX THIS
        feature_counts = []
        for symbol in symbols:
            parquet_file_path = feature_results['storage_paths'][symbol]
            response = self.minio_client.get_object(self.artifact_bucket, parquet_file_path)
            df = pd.read_parquet(BytesIO(response.read()))
            response.close()
            response.release_conn()
            feature_counts.append(len(df.columns))  # Now df has .columns
        
        axes[0,0].bar(symbols, feature_counts)
        axes[0,0].set_title('Feature Count by Symbol')
        axes[0,0].set_ylabel('Number of Features')
        
        # 2. Feature type distribution - FIX THIS
        first_symbol = symbols[0]
        parquet_file_path = feature_results['storage_paths'][first_symbol]
        response = self.minio_client.get_object(self.artifact_bucket, parquet_file_path)
        first_df = pd.read_parquet(BytesIO(response.read()))
        response.close()
        response.release_conn()
        feature_types = self._categorize_features(first_df.columns)
        
        axes[0,1].pie(feature_types.values(), labels=feature_types.keys(), autopct='%1.1f%%')
        axes[0,1].set_title(f'Feature Type Distribution ({first_symbol})')
        
        # 3. Null value heatmap
        null_data = []
        for symbol in symbols:
            parquet_file_path = feature_results['storage_paths'][symbol]
            response = self.minio_client.get_object(self.artifact_bucket, parquet_file_path)
            df = pd.read_parquet(BytesIO(response.read()))
            response.close()
            response.release_conn()
            null_pct = (df.isnull().sum() / len(df) * 100).mean()
            null_data.append(null_pct)
        
        axes[1,0].bar(symbols, null_data, color=['red' if x > 10 else 'orange' if x > 5 else 'green' for x in null_data])
        axes[1,0].set_title('Average Null Percentage by Symbol')
        axes[1,0].set_ylabel('Null Percentage (%)')
        
        # 4. Feature correlation summary
        high_corr_counts = []
        for symbol in symbols:
            parquet_file_path = feature_results['storage_paths'][symbol]
            response = self.minio_client.get_object(self.artifact_bucket, parquet_file_path)
            df = pd.read_parquet(BytesIO(response.read()))
            response.close()
            response.release_conn()
            corr_matrix = df.select_dtypes(include=[np.number]).corr()
            high_corr_pairs = self._find_high_correlations(corr_matrix)
            high_corr_counts.append(len(high_corr_pairs))
        
        axes[1,1].bar(symbols, high_corr_counts)
        axes[1,1].set_title('High Correlation Feature Pairs')
        axes[1,1].set_ylabel('Number of High Corr Pairs (>0.8)')
        
        plt.tight_layout()
        # Save to MinIO instead of MLflow
        img_buffer = BytesIO()
        fig.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')
        img_buffer.seek(0)
        
        object_name = f"mlflow-runs/{run_id}/artifacts/feature_engineering_overview.png"
        self.minio_client.put_object(
            bucket_name=self.artifact_bucket,
            object_name=object_name,
            data=img_buffer,
            length=len(img_buffer.getvalue()),
            content_type='image/png'
        )
        
        plt.close()
        self.logger.info(f"Saved visualization to MinIO: {object_name}")
        
        # Create detailed correlation heatmap for first symbol
        parquet_file_path = feature_results['storage_paths'][symbols[0]]
        response = self.minio_client.get_object(self.artifact_bucket, parquet_file_path)
        first_symbol_df = pd.read_parquet(BytesIO(response.read()))
        response.close()
        response.release_conn()
        self._create_correlation_heatmap(first_symbol_df, symbols[0], run_id)
    
    def _create_correlation_heatmap(self, df, symbol, run_id):
        """Create correlation heatmap for features"""
        # Select subset of features for readability
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        sample_cols = numeric_cols[:20] if len(numeric_cols) > 20 else numeric_cols
        
        fig = plt.figure(figsize=(12, 10))
        corr_matrix = df[sample_cols].corr()
        
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', 
                cmap='coolwarm', center=0, square=True)
        plt.title(f'Feature Correlation Matrix - {symbol} (Sample)')
        plt.tight_layout()
        
        # Save to MinIO instead of MLflow
        img_buffer = BytesIO()
        fig.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')
        img_buffer.seek(0)
        
        object_name = f"mlflow-runs/{run_id}/artifacts/correlation_matrix_{symbol}.png"
        self.minio_client.put_object(
            bucket_name=self.artifact_bucket,
            object_name=object_name,
            data=img_buffer,
            length=len(img_buffer.getvalue()),
            content_type='image/png'
        )
        
        plt.close()
        self.logger.info(f"Saved correlation heatmap to MinIO: {object_name}")
    
    def _log_feature_statistics(self, feature_results, symbols, run_id):
        """Log detailed feature statistics"""
        for symbol in symbols:
            parquet_file_path = feature_results['storage_paths'][symbol]
            response = self.minio_client.get_object(self.artifact_bucket, parquet_file_path)
            df = pd.read_parquet(BytesIO(response.read()))
            response.close()
            response.release_conn()
            
            # Basic statistics for numeric features
            numeric_df = df.select_dtypes(include=[np.number])
            stats = numeric_df.describe()
            
            # Save to MinIO instead of MLflow
            stats_dict = stats.to_dict()
            json_data = json.dumps(stats_dict, indent=2, default=str)
            json_buffer = BytesIO(json_data.encode('utf-8'))
            
            object_name = f"mlflow-runs/{run_id}/artifacts/feature_statistics_{symbol}.json"
            self.minio_client.put_object(
                bucket_name=self.artifact_bucket,
                object_name=object_name,
                data=json_buffer,
                length=len(json_data.encode('utf-8')),
                content_type='application/json'
            )
            
            # Log key statistics as metrics
            mlflow.log_metric(f"feature_mean_std_{symbol}", stats.loc['std'].mean())
            mlflow.log_metric(f"feature_mean_skew_{symbol}", numeric_df.skew().mean())
    

# Extended monitoring class that includes validation
class ValidationMonitoring(MLMonitoringIntegration):
    def __init__(self, db_config, **kwargs):
        super().__init__(**kwargs)
        self.db_config = db_config
    """Extended monitoring that includes validation monitoring"""
    
    def _create_validation_dashboard(self, raw_results, feature_results, symbols, run_id):
        """Create comprehensive validation dashboard"""
        
        # Create 2x3 subplot layout
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. Overall pass rates
        raw_pass_rates = []
        feature_pass_rates = []
        
        for symbol in symbols:
            if raw_results and symbol in raw_results:
                raw_rate = raw_results[symbol]['passed_rules'] / raw_results[symbol]['total_rules'] if raw_results[symbol]['total_rules'] > 0 else 0
                raw_pass_rates.append(raw_rate * 100)
            else:
                raw_pass_rates.append(0)
                
            if feature_results and symbol in feature_results:
                feature_rate = feature_results[symbol]['passed_rules'] / feature_results[symbol]['total_rules'] if feature_results[symbol]['total_rules'] > 0 else 0
                feature_pass_rates.append(feature_rate * 100)
            else:
                feature_pass_rates.append(0)
        
        x = np.arange(len(symbols))
        width = 0.35
        
        axes[0,0].bar(x - width/2, raw_pass_rates, width, label='Raw Data', alpha=0.8)
        axes[0,0].bar(x + width/2, feature_pass_rates, width, label='Features', alpha=0.8)
        axes[0,0].set_xlabel('Symbols')
        axes[0,0].set_ylabel('Pass Rate (%)')
        axes[0,0].set_title('Validation Pass Rates by Symbol')
        axes[0,0].set_xticks(x)
        axes[0,0].set_xticklabels(symbols, rotation=45)
        axes[0,0].legend()
        axes[0,0].set_ylim(0, 100)
        
        # 2. Critical failures heatmap
        critical_data = []
        for symbol in symbols:
            raw_critical = raw_results[symbol]['critical_failures'] if raw_results and symbol in raw_results else 0
            feature_critical = feature_results[symbol]['critical_failures'] if feature_results and symbol in feature_results else 0
            critical_data.append([raw_critical, feature_critical])
        
        critical_df = pd.DataFrame(critical_data, columns=['Raw Data', 'Features'], index=symbols)
        sns.heatmap(critical_df.T, annot=True, fmt='d', ax=axes[0,1], cmap='Reds', cbar_kws={'label': 'Critical Failures'})
        axes[0,1].set_title('Critical Validation Failures')
        
        # 3. Warnings distribution
        warning_data = []
        for symbol in symbols:
            raw_warnings = raw_results[symbol]['warnings'] if raw_results and symbol in raw_results else 0
            feature_warnings = feature_results[symbol]['warnings'] if feature_results and symbol in feature_results else 0
            warning_data.append([raw_warnings, feature_warnings])
        
        warning_df = pd.DataFrame(warning_data, columns=['Raw Data', 'Features'], index=symbols)
        sns.heatmap(warning_df.T, annot=True, fmt='d', ax=axes[0,2], cmap='Oranges', cbar_kws={'label': 'Warnings'})
        axes[0,2].set_title('Validation Warnings')
        
        # 4. Validation trend over time (last 7 days)
        historical_data = self._get_validation_history(days=7)
        if historical_data:
            hist_df = pd.DataFrame(historical_data)
            hist_df['validation_time'] = pd.to_datetime(hist_df['validation_time'])
            hist_df['pass_rate'] = hist_df['passed_count'] / hist_df['rule_count']
            
            # Group by date and calculate average pass rate
            daily_stats = hist_df.groupby(hist_df['validation_time'].dt.date)['pass_rate'].mean().reset_index()
            
            axes[1,0].plot(daily_stats['validation_time'], daily_stats['pass_rate'] * 100, marker='o')
            axes[1,0].set_xlabel('Date')
            axes[1,0].set_ylabel('Average Pass Rate (%)')
            axes[1,0].set_title('Validation Trend (7 Days)')
            axes[1,0].tick_params(axis='x', rotation=45)
        else:
            axes[1,0].text(0.5, 0.5, 'No Historical Data', ha='center', va='center', transform=axes[1,0].transAxes)
            axes[1,0].set_title('Validation Trend (No Data)')
        
        # 5. Rule failure breakdown
        rule_failures = {}
        for symbol in symbols:
            if raw_results and symbol in raw_results:
                for result in raw_results[symbol]['results']:
                    if not result['passed']:
                        rule_name = result['rule_name']
                        rule_failures[rule_name] = rule_failures.get(rule_name, 0) + 1
            
            if feature_results and symbol in feature_results:
                for result in feature_results[symbol]['results']:
                    if not result['passed']:
                        rule_name = result['rule_name']
                        rule_failures[rule_name] = rule_failures.get(rule_name, 0) + 1
        
        if rule_failures:
            rules = list(rule_failures.keys())[:10]  # Top 10 failing rules
            counts = [rule_failures[rule] for rule in rules]
            
            axes[1,1].barh(rules, counts)
            axes[1,1].set_xlabel('Failure Count')
            axes[1,1].set_title('Most Common Rule Failures')
        else:
            axes[1,1].text(0.5, 0.5, 'No Rule Failures', ha='center', va='center', transform=axes[1,1].transAxes)
            axes[1,1].set_title('Rule Failures (None)')
        
        # 6. Overall health score
        overall_scores = []
        for symbol in symbols:
            raw_score = raw_pass_rates[symbols.index(symbol)] if raw_results else 100
            feature_score = feature_pass_rates[symbols.index(symbol)] if feature_results else 100
            overall_score = (raw_score + feature_score) / 2
            overall_scores.append(overall_score)
        
        colors = ['green' if s >= 90 else 'orange' if s >= 70 else 'red' for s in overall_scores]
        axes[1,2].bar(symbols, overall_scores, color=colors, alpha=0.7)
        axes[1,2].set_xlabel('Symbols')
        axes[1,2].set_ylabel('Health Score (%)')
        axes[1,2].set_title('Overall Data Health Score')
        axes[1,2].tick_params(axis='x', rotation=45)
        axes[1,2].set_ylim(0, 100)
        
        # Add horizontal lines for thresholds
        axes[1,2].axhline(y=90, color='green', linestyle='--', alpha=0.5, label='Excellent')
        axes[1,2].axhline(y=70, color='orange', linestyle='--', alpha=0.5, label='Good')
        axes[1,2].legend()
        
        plt.tight_layout()
        
        # Save to MinIO
        img_buffer = BytesIO()
        fig.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')
        img_buffer.seek(0)
        
        object_name = f"mlflow-runs/{run_id}/artifacts/validation_dashboard.png"
        self.minio_client.put_object(
            bucket_name='crypto-features',
            object_name=object_name,
            data=img_buffer,
            length=len(img_buffer.getvalue()),
            content_type='image/png'
        )
        
        plt.close()
        self.logger.info(f"Saved validation dashboard to MinIO: {object_name}")
    
    def _save_validation_summary(self, raw_results, feature_results, run_id):
        """Save validation summary to MinIO"""
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'raw_data_validation': raw_results,
            'feature_validation': feature_results,
            'overall_stats': {
                'total_symbols': len(set(list(raw_results.keys() if raw_results else []) + list(feature_results.keys() if feature_results else []))),
                'total_critical_failures': sum(r['critical_failures'] for r in (raw_results or {}).values()) + sum(r['critical_failures'] for r in (feature_results or {}).values()),
                'total_warnings': sum(r['warnings'] for r in (raw_results or {}).values()) + sum(r['warnings'] for r in (feature_results or {}).values())
            }
        }
        
        # Convert to JSON bytes
        json_data = json.dumps(summary, indent=2, default=str)
        json_buffer = BytesIO(json_data.encode('utf-8'))
        
        # Save to MinIO
        object_name = f"mlflow-runs/{run_id}/artifacts/validation_summary.json"
        self.minio_client.put_object(
            bucket_name='crypto-features',
            object_name=object_name,
            data=json_buffer,
            length=len(json_data.encode('utf-8')),
            content_type='application/json'
        )
        
        self.logger.info(f"Saved validation summary to MinIO: {object_name}")
    
    def _get_validation_history(self, days=7):
        """Get validation history from database"""
        query = """
        SELECT dataset_name, symbol, validation_time, severity, 
               COUNT(*) as rule_count,
               SUM(CASE WHEN passed THEN 1 ELSE 0 END) as passed_count
        FROM validation_results 
        WHERE validation_time >= NOW() - INTERVAL '%s days'
        GROUP BY dataset_name, symbol, validation_time, severity 
        ORDER BY validation_time DESC
        """
        
        try:
            with psycopg2.connect(**self.db_config) as conn:
                df = pd.read_sql(query, conn, params=[days])
            return df.to_dict('records')
        except Exception as e:
            self.logger.error(f"Failed to get validation history: {str(e)}")
            return []

    def log_validation_metrics(self, raw_validation_results, feature_validation_results, symbols, run_id):
        """Log validation results to MLflow and Prometheus"""
        
        # Log raw data validation metrics
        if raw_validation_results:
            for symbol, result in raw_validation_results.items():
                pass_rate = result['passed_rules'] / result['total_rules'] if result['total_rules'] > 0 else 0
                
                # MLflow metrics
                mlflow.log_metric(f"raw_validation_pass_rate_{symbol}", pass_rate)
                mlflow.log_metric(f"raw_validation_critical_{symbol}", result['critical_failures'])
                mlflow.log_metric(f"raw_validation_warnings_{symbol}", result['warnings'])
                
                # Prometheus metrics
                self.validation_pass_rate.labels(dataset_name="raw_data", symbol=symbol).set(pass_rate)
                self.validation_critical_failures.labels(dataset_name="raw_data", symbol=symbol).set(result['critical_failures'])
                self.validation_warnings.labels(dataset_name="raw_data", symbol=symbol).set(result['warnings'])
                
                # Count runs
                status = "success" if result['critical_failures'] == 0 else "failed"
                self.validation_runs_total.labels(dataset_name="raw_data", status=status).inc()
        
        # Log feature validation metrics
        if feature_validation_results:
            for symbol, result in feature_validation_results.items():
                pass_rate = result['passed_rules'] / result['total_rules'] if result['total_rules'] > 0 else 0
                
                # MLflow metrics
                mlflow.log_metric(f"feature_validation_pass_rate_{symbol}", pass_rate)
                mlflow.log_metric(f"feature_validation_critical_{symbol}", result['critical_failures'])
                mlflow.log_metric(f"feature_validation_warnings_{symbol}", result['warnings'])
                
                # Prometheus metrics
                self.validation_pass_rate.labels(dataset_name="features", symbol=symbol).set(pass_rate)
                self.validation_critical_failures.labels(dataset_name="features", symbol=symbol).set(result['critical_failures'])
                self.validation_warnings.labels(dataset_name="features", symbol=symbol).set(result['warnings'])
                
                # Count runs
                status = "success" if result['critical_failures'] == 0 else "failed"
                self.validation_runs_total.labels(dataset_name="features", status=status).inc()
        
        # Create validation dashboard visualizations
        self._create_validation_dashboard(raw_validation_results, feature_validation_results, symbols, run_id)
        
        # Save validation summary
        self._save_validation_summary(raw_validation_results, feature_validation_results, run_id)
      
    def log_comprehensive_metrics(self, quality_results, feature_results, validation_results, symbols):
        """Log all monitoring metrics including validation"""
        
        # Start MLflow run
        experiment_name = "comprehensive_ml_monitoring"
        try:
            experiment = mlflow.get_experiment_by_name(experiment_name)
            if experiment is None:
                mlflow.create_experiment(experiment_name)
        except:
            mlflow.create_experiment(experiment_name)
        
        mlflow.set_experiment(experiment_name)
        run = mlflow.start_run(run_name=f"comprehensive_monitoring_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        run_id = run.info.run_id
        
        try:
            # Log data quality metrics (your existing code)
            if quality_results:
                overall_completeness = quality_results['completeness']['avg_completeness']
                mlflow.log_metric("overall_completeness_ratio", overall_completeness)
            
            # Log feature engineering metrics (your existing code)
            if feature_results:
                total_features = sum(len(pd.read_parquet(BytesIO(self.minio_client.get_object('crypto-features', path).read())).columns) 
                                   for path in feature_results['storage_paths'].values())
                mlflow.log_metric("total_features_generated", total_features)
            
            # Log validation metrics (NEW)
            if validation_results:
                raw_validation = validation_results.get('raw_validation', {})
                feature_validation = validation_results.get('feature_validation', {})
                
                self.log_validation_metrics(
                    raw_validation, feature_validation, symbols, run_id
                )
            
            self.logger.info("Comprehensive monitoring metrics logged successfully")
            
        finally:
            mlflow.end_run()
        
        # Push to Prometheus
        try:
            from prometheus_client import push_to_gateway
            push_to_gateway(self.prometheus_gateway, job='comprehensive_ml_monitoring', registry=self.registry)
            self.logger.info("Pushed comprehensive metrics to Prometheus")
        except Exception as e:
            self.logger.error(f"Failed to push to Prometheus: {e}")